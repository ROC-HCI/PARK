{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_row_id</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>date</th>\n",
       "      <th>protocol</th>\n",
       "      <th>test_split</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>race</th>\n",
       "      <th>filename_smile</th>\n",
       "      <th>...</th>\n",
       "      <th>misclassified_smile</th>\n",
       "      <th>misclassified_speech</th>\n",
       "      <th>misclassified_finger</th>\n",
       "      <th>misclassified_fusion</th>\n",
       "      <th>uncertain_flag</th>\n",
       "      <th>pred_std_fusion</th>\n",
       "      <th>neurologist_label_ray</th>\n",
       "      <th>neurologist_label_ruth</th>\n",
       "      <th>neurologist_label_jamie</th>\n",
       "      <th>pred_park</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NIHNT179KNNF4#2022-03-24</td>\n",
       "      <td>NIHNT179KNNF4</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>SuperPD</td>\n",
       "      <td>global</td>\n",
       "      <td>Female</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60 - 79</td>\n",
       "      <td>White</td>\n",
       "      <td>2022-03-24T13%3A32%3A36.977Z_NIHNT179KNNF4_smi...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIHNT179KNNF4#2023-06-30</td>\n",
       "      <td>NIHNT179KNNF4</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>SuperPD</td>\n",
       "      <td>global</td>\n",
       "      <td>Female</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60 - 79</td>\n",
       "      <td>White</td>\n",
       "      <td>2023-06-30T15%3A13%3A51.098Z_NIHNT179KNNF4_smi...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.032354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIHNT823CHAC3#2022-05-20</td>\n",
       "      <td>NIHNT823CHAC3</td>\n",
       "      <td>2022-05-20</td>\n",
       "      <td>SuperPD</td>\n",
       "      <td>global</td>\n",
       "      <td>Female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>60 - 79</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>2022-05-20T19%3A15%3A10.454Z_NIHNT823CHAC3_smi...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NIHNT823CHAC3#2021-05-07</td>\n",
       "      <td>NIHNT823CHAC3</td>\n",
       "      <td>2021-05-07</td>\n",
       "      <td>SuperPD</td>\n",
       "      <td>global</td>\n",
       "      <td>Female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>60 - 79</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>NIHNT823CHAC3-smile-2021-05-07T21-05-27-387Z-.mp4</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.023983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NIHNT823CHAC3#2021-11-01</td>\n",
       "      <td>NIHNT823CHAC3</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>SuperPD</td>\n",
       "      <td>global</td>\n",
       "      <td>Female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>60 - 79</td>\n",
       "      <td>Black or African American</td>\n",
       "      <td>NIHNT823CHAC3-smile-2021-11-01T19-07-54-512Z-.mp4</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.013366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              unique_row_id participant_id        date protocol test_split  \\\n",
       "0  NIHNT179KNNF4#2022-03-24  NIHNT179KNNF4  2022-03-24  SuperPD     global   \n",
       "1  NIHNT179KNNF4#2023-06-30  NIHNT179KNNF4  2023-06-30  SuperPD     global   \n",
       "2  NIHNT823CHAC3#2022-05-20  NIHNT823CHAC3  2022-05-20  SuperPD     global   \n",
       "3  NIHNT823CHAC3#2021-05-07  NIHNT823CHAC3  2021-05-07  SuperPD     global   \n",
       "4  NIHNT823CHAC3#2021-11-01  NIHNT823CHAC3  2021-11-01  SuperPD     global   \n",
       "\n",
       "   gender   age age_group                       race  \\\n",
       "0  Female  70.0   60 - 79                      White   \n",
       "1  Female  70.0   60 - 79                      White   \n",
       "2  Female  62.0   60 - 79  Black or African American   \n",
       "3  Female  62.0   60 - 79  Black or African American   \n",
       "4  Female  62.0   60 - 79  Black or African American   \n",
       "\n",
       "                                      filename_smile  ... misclassified_smile  \\\n",
       "0  2022-03-24T13%3A32%3A36.977Z_NIHNT179KNNF4_smi...  ...               False   \n",
       "1  2023-06-30T15%3A13%3A51.098Z_NIHNT179KNNF4_smi...  ...                True   \n",
       "2  2022-05-20T19%3A15%3A10.454Z_NIHNT823CHAC3_smi...  ...                True   \n",
       "3  NIHNT823CHAC3-smile-2021-05-07T21-05-27-387Z-.mp4  ...                True   \n",
       "4  NIHNT823CHAC3-smile-2021-11-01T19-07-54-512Z-.mp4  ...                True   \n",
       "\n",
       "  misclassified_speech misclassified_finger misclassified_fusion  \\\n",
       "0                False                False                False   \n",
       "1                False                 True                False   \n",
       "2                 True                 True                 True   \n",
       "3                False                 True                 True   \n",
       "4                False                False                False   \n",
       "\n",
       "   uncertain_flag  pred_std_fusion  neurologist_label_ray  \\\n",
       "0           False         0.001296                    1.0   \n",
       "1           False         0.032354                    1.0   \n",
       "2           False         0.003919                    0.0   \n",
       "3           False         0.023983                    NaN   \n",
       "4           False         0.013366                    NaN   \n",
       "\n",
       "   neurologist_label_ruth  neurologist_label_jamie  pred_park  \n",
       "0                     0.0                      0.0          1  \n",
       "1                     1.0                      0.0          1  \n",
       "2                     0.0                      0.0          1  \n",
       "3                     NaN                      NaN          1  \n",
       "4                     NaN                      NaN          0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_results = pd.read_csv('../../data/test_data_big.csv')\n",
    "df_test_results['pred_park'] = (df_test_results['pred_score_fusion'] >= 0.5).astype(int)\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def calculate_metrics_bootstrapped(true_labels, pred_labels, n_bootstraps=100):\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "\n",
    "    # Lists to store bootstrap results\n",
    "    boot_auroc, boot_acc, boot_ppv, boot_npv, boot_sens, boot_spec, boot_f1 = [], [], [], [], [], [], []\n",
    "\n",
    "    boot_count = 0\n",
    "    while boot_count < n_bootstraps:\n",
    "        # Sample indices with replacement\n",
    "        indices = rng.choice(len(true_labels), size=len(true_labels), replace=True)\n",
    "        \n",
    "        # Sample labels\n",
    "        y_true = true_labels[indices]\n",
    "        y_pred = pred_labels[indices]\n",
    "\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            continue  # Retry this bootstrap\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        try:\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        except ValueError:\n",
    "            continue  # Retry this bootstrap\n",
    "\n",
    "        # Compute metrics safely\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        f1_score = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else np.nan\n",
    "\n",
    "        if np.isnan([ppv, npv, sensitivity, specificity, f1_score]).any():\n",
    "            continue  # Retry this bootstrap\n",
    "\n",
    "        # Append to result lists\n",
    "        boot_auroc.append(auroc)\n",
    "        boot_acc.append(acc)\n",
    "        boot_ppv.append(ppv)\n",
    "        boot_npv.append(npv)\n",
    "        boot_sens.append(sensitivity)\n",
    "        boot_spec.append(specificity)\n",
    "        boot_f1.append(f1_score)\n",
    "\n",
    "        boot_count += 1  # only increment if the sample is valid\n",
    "\n",
    "    # Aggregate results\n",
    "    bootstrap_results = {\n",
    "        'AUROC': boot_auroc,\n",
    "        'Accuracy': boot_acc,\n",
    "        'PPV': boot_ppv,\n",
    "        'NPV': boot_npv,\n",
    "        'Sensitivity': boot_sens,\n",
    "        'Specificity': boot_spec,\n",
    "        'F1 Score': boot_f1\n",
    "    }\n",
    "\n",
    "    # Summary with mean ± 95% CI\n",
    "    summary = {}\n",
    "    for metric, values in bootstrap_results.items():\n",
    "        mean_val = np.nanmean(values)\n",
    "        lower_ci = np.nanpercentile(values, 2.5)\n",
    "        upper_ci = np.nanpercentile(values, 97.5)\n",
    "        margin = (upper_ci - lower_ci) / 2\n",
    "        summary[metric] = f\"{round(mean_val * 100, 1)} ± {round(margin * 100, 1)}\"\n",
    "\n",
    "    summary_df = pd.DataFrame.from_dict(summary, orient='index', columns=['Mean ± 95% CI'])\n",
    "    return summary_df, bootstrap_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_split\n",
       "global          162\n",
       "validation_1     91\n",
       "validation_2     67\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_results['test_split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean ± 95% CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>79.2 ± 6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>80.7 ± 5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPV</th>\n",
       "      <td>81.8 ± 7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPV</th>\n",
       "      <td>78.7 ± 9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>86.7 ± 5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>71.7 ± 12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>84.1 ± 5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mean ± 95% CI\n",
       "AUROC          79.2 ± 6.6\n",
       "Accuracy       80.7 ± 5.9\n",
       "PPV            81.8 ± 7.1\n",
       "NPV            78.7 ± 9.0\n",
       "Sensitivity    86.7 ± 5.7\n",
       "Specificity   71.7 ± 12.7\n",
       "F1 Score       84.1 ± 5.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_global = df_test_results[df_test_results['test_split'] == 'global']\n",
    "\n",
    "true_labels_global = np.asarray(df_global['true_label'])\n",
    "pred_labels_global = np.asarray(df_global['pred_park'])   \n",
    "\n",
    "summary_df_global, bootstrap_results_global = calculate_metrics_bootstrapped(true_labels_global, pred_labels_global)\n",
    "summary_df_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean ± 95% CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>79.6 ± 8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>80.1 ± 8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPV</th>\n",
       "      <td>80.1 ± 10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPV</th>\n",
       "      <td>80.2 ± 12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>84.8 ± 10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>74.5 ± 14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>82.2 ± 7.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mean ± 95% CI\n",
       "AUROC          79.6 ± 8.5\n",
       "Accuracy       80.1 ± 8.2\n",
       "PPV           80.1 ± 10.8\n",
       "NPV           80.2 ± 12.0\n",
       "Sensitivity   84.8 ± 10.7\n",
       "Specificity   74.5 ± 14.3\n",
       "F1 Score       82.2 ± 7.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_1 = df_test_results[df_test_results['test_split'] == 'validation_1']\n",
    "true_labels_val_1 = np.asarray(df_val_1['true_label'])\n",
    "pred_labels_val_1 = np.asarray(df_val_1['pred_park'])   \n",
    "\n",
    "summary_df_val_1, bootstrap_results_val_1 = calculate_metrics_bootstrapped(true_labels_val_1, pred_labels_val_1)\n",
    "summary_df_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean ± 95% CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>81.4 ± 9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>81.0 ± 8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPV</th>\n",
       "      <td>75.9 ± 13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPV</th>\n",
       "      <td>86.0 ± 12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>84.2 ± 13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>78.5 ± 13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>79.6 ± 10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mean ± 95% CI\n",
       "AUROC          81.4 ± 9.1\n",
       "Accuracy       81.0 ± 8.6\n",
       "PPV           75.9 ± 13.9\n",
       "NPV           86.0 ± 12.6\n",
       "Sensitivity   84.2 ± 13.5\n",
       "Specificity   78.5 ± 13.5\n",
       "F1 Score      79.6 ± 10.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_2 = df_test_results[df_test_results['test_split'] == 'validation_2']\n",
    "\n",
    "true_labels_val_2 = np.asarray(df_val_2['true_label'])\n",
    "pred_labels_val_2 = np.asarray(df_val_2['pred_park'])   \n",
    "\n",
    "summary_df_val_2, bootstrap_results_val_2 = calculate_metrics_bootstrapped(true_labels_val_2, pred_labels_val_2)\n",
    "summary_df_val_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison</th>\n",
       "      <th>U Statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced Test Set vs Validation Study 1</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>1.299756e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced Test Set vs Validation Study 2</td>\n",
       "      <td>7541.5</td>\n",
       "      <td>5.333058e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Validation Study 1 vs Validation Study 2</td>\n",
       "      <td>6684.5</td>\n",
       "      <td>3.866663e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Comparison  U Statistic       p-value\n",
       "0   Balanced Test Set vs Validation Study 1       6017.0  1.299756e-02\n",
       "1   Balanced Test Set vs Validation Study 2       7541.5  5.333058e-10\n",
       "2  Validation Study 1 vs Validation Study 2       6684.5  3.866663e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "# from statsmodels.stats.multitest import multipletests\n",
    "import itertools\n",
    "\n",
    "# Define the distributions (re-using group_a, group_b, group_c as stand-ins)\n",
    "distribution_sets = [\n",
    "    bootstrap_results_global, \n",
    "    bootstrap_results_val_1,\n",
    "    bootstrap_results_val_2\n",
    "]\n",
    "labels = ['Balanced Test Set', 'Validation Study 1', 'Validation Study 2']\n",
    "metric_name = 'PPV'\n",
    "\n",
    "# Perform pairwise Mann-Whitney U tests\n",
    "results = []\n",
    "\n",
    "for (i, j) in itertools.combinations(range(len(distribution_sets)), 2):\n",
    "    group1 = distribution_sets[i][metric_name]\n",
    "    group2 = distribution_sets[j][metric_name]\n",
    "    label1 = labels[i]\n",
    "    label2 = labels[j]\n",
    "    stat, p = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    results.append({\n",
    "        'Comparison': f'{label1} vs {label2}',\n",
    "        'U Statistic': stat,\n",
    "        'p-value': p\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison</th>\n",
       "      <th>U Statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balanced Test Set vs Validation Study 1</td>\n",
       "      <td>6139.0</td>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Balanced Test Set vs Validation Study 2</td>\n",
       "      <td>6421.5</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Validation Study 1 vs Validation Study 2</td>\n",
       "      <td>5358.5</td>\n",
       "      <td>0.381538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Comparison  U Statistic   p-value\n",
       "0   Balanced Test Set vs Validation Study 1       6139.0  0.005398\n",
       "1   Balanced Test Set vs Validation Study 2       6421.5  0.000516\n",
       "2  Validation Study 1 vs Validation Study 2       5358.5  0.381538"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Define the distributions (re-using group_a, group_b, group_c as stand-ins)\n",
    "distribution_sets = [\n",
    "    bootstrap_results_global, \n",
    "    bootstrap_results_val_1,\n",
    "    bootstrap_results_val_2\n",
    "]\n",
    "labels = ['Balanced Test Set', 'Validation Study 1', 'Validation Study 2']\n",
    "metric_name = 'Sensitivity'\n",
    "\n",
    "# Perform pairwise Mann-Whitney U tests\n",
    "results = []\n",
    "\n",
    "for (i, j) in itertools.combinations(range(len(distribution_sets)), 2):\n",
    "    group1 = distribution_sets[i][metric_name]\n",
    "    group2 = distribution_sets[j][metric_name]\n",
    "    label1 = labels[i]\n",
    "    label2 = labels[j]\n",
    "    stat, p = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "    results.append({\n",
    "        'Comparison': f'{label1} vs {label2}',\n",
    "        'U Statistic': stat,\n",
    "        'p-value': p\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9401730733233826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.9401730733233826, 0.157)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Observed counts (correct, incorrect) per PD stage\n",
    "data = np.array([\n",
    "    [11, 2],   # Stage 1\n",
    "    [43, 4],   # Stage 2\n",
    "    [8, 3]     # Stage 3\n",
    "])\n",
    "\n",
    "# Compute observed chi-square statistic\n",
    "observed_stat, _, _, _ = chi2_contingency(data, correction=False)\n",
    "\n",
    "print(observed_stat)\n",
    "\n",
    "# Flatten into array of 1s and 0s\n",
    "flat = np.concatenate([[1]*c + [0]*i for c, i in data])\n",
    "group_sizes = data.sum(axis=1)\n",
    "\n",
    "# Monte Carlo simulation\n",
    "n_sim = 10000\n",
    "simulated_stats = []\n",
    "\n",
    "for _ in range(n_sim):\n",
    "    shuffled = np.random.permutation(flat)\n",
    "    reshaped = []\n",
    "    start = 0\n",
    "    for size in group_sizes:\n",
    "        group = shuffled[start:start+size]\n",
    "        correct = (group == 1).sum()\n",
    "        incorrect = size - correct\n",
    "        reshaped.append([correct, incorrect])\n",
    "        start += size\n",
    "    reshaped = np.array(reshaped)\n",
    "    stat, _, _, _ = chi2_contingency(reshaped, correction=False)\n",
    "    simulated_stats.append(stat)\n",
    "\n",
    "simulated_stats = np.array(simulated_stats)\n",
    "p_mc = (simulated_stats >= observed_stat).mean()\n",
    "\n",
    "observed_stat, p_mc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {'gender': [{'group': 'Female',\n",
    "   'mean': 0.22,\n",
    "   'se': 0.03,\n",
    "   'ci_lower': 0.16,\n",
    "   'ci_upper': 0.29,\n",
    "   'n_total': 171,\n",
    "   'n_wrong': 38},\n",
    "  {'group': 'Male',\n",
    "   'mean': 0.17,\n",
    "   'se': 0.03,\n",
    "   'ci_lower': 0.11,\n",
    "   'ci_upper': 0.23,\n",
    "   'n_total': 147,\n",
    "   'n_wrong': 25},\n",
    "  {'group': 'Unknown',\n",
    "   'mean': 0.0,\n",
    "   'se': 0.0,\n",
    "   'ci_lower': 0.0,\n",
    "   'ci_upper': 0.0,\n",
    "   'n_total': 2,\n",
    "   'n_wrong': 0},\n",
    "  {'group': 'all',\n",
    "   'mean': 0.2,\n",
    "   'se': 0.02,\n",
    "   'ci_lower': 0.15,\n",
    "   'ci_upper': 0.24,\n",
    "   'n_total': 320,\n",
    "   'n_wrong': 63}],\n",
    " 'age_group': [{'group': '60 - 79',\n",
    "   'mean': 0.2,\n",
    "   'se': 0.03,\n",
    "   'ci_lower': 0.14,\n",
    "   'ci_upper': 0.25,\n",
    "   'n_total': 194,\n",
    "   'n_wrong': 39},\n",
    "  {'group': '40 - 59',\n",
    "   'mean': 0.18,\n",
    "   'se': 0.04,\n",
    "   'ci_lower': 0.1,\n",
    "   'ci_upper': 0.26,\n",
    "   'n_total': 85,\n",
    "   'n_wrong': 15},\n",
    "  {'group': 'Not Mentioned',\n",
    "   'mean': 0.0,\n",
    "   'se': 0.0,\n",
    "   'ci_lower': 0.0,\n",
    "   'ci_upper': 0.0,\n",
    "   'n_total': 3,\n",
    "   'n_wrong': 0},\n",
    "  {'group': '>= 80',\n",
    "   'mean': 0.0,\n",
    "   'se': 0.0,\n",
    "   'ci_lower': 0.0,\n",
    "   'ci_upper': 0.0,\n",
    "   'n_total': 7,\n",
    "   'n_wrong': 0},\n",
    "  {'group': '20 - 39',\n",
    "   'mean': 0.33,\n",
    "   'se': 0.09,\n",
    "   'ci_lower': 0.16,\n",
    "   'ci_upper': 0.5,\n",
    "   'n_total': 27,\n",
    "   'n_wrong': 9},\n",
    "  {'group': '< 20',\n",
    "   'mean': 0.0,\n",
    "   'se': 0.0,\n",
    "   'ci_lower': 0.0,\n",
    "   'ci_upper': 0.0,\n",
    "   'n_total': 4,\n",
    "   'n_wrong': 0},\n",
    "  {'group': 'all',\n",
    "   'mean': 0.2,\n",
    "   'se': 0.02,\n",
    "   'ci_lower': 0.15,\n",
    "   'ci_upper': 0.24,\n",
    "   'n_total': 320,\n",
    "   'n_wrong': 63}],\n",
    " 'race': [{'group': 'white',\n",
    "   'mean': 0.18,\n",
    "   'se': 0.03,\n",
    "   'ci_lower': 0.13,\n",
    "   'ci_upper': 0.23,\n",
    "   'n_total': 226,\n",
    "   'n_wrong': 40},\n",
    "  {'group': 'Non-white',\n",
    "   'mean': 0.24,\n",
    "   'se': 0.06,\n",
    "   'ci_lower': 0.12,\n",
    "   'ci_upper': 0.35,\n",
    "   'n_total': 59,\n",
    "   'n_wrong': 14},\n",
    "  {'group': 'Unknown',\n",
    "   'mean': 0.26,\n",
    "   'se': 0.07,\n",
    "   'ci_lower': 0.12,\n",
    "   'ci_upper': 0.4,\n",
    "   'n_total': 35,\n",
    "   'n_wrong': 9},\n",
    "  {'group': 'all',\n",
    "   'mean': 0.2,\n",
    "   'se': 0.02,\n",
    "   'ci_lower': 0.15,\n",
    "   'ci_upper': 0.24,\n",
    "   'n_total': 320,\n",
    "   'n_wrong': 63}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 'z-test for two proportions',\n",
       " 'z_statistic': 1.1634,\n",
       " 'p_value': 0.2447,\n",
       " 'female_error_rate': 0.222,\n",
       " 'male_error_rate': 0.17,\n",
       " 'sample_size_conditions_met': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "# Extract gender data for male and female only\n",
    "gender_data = summary['gender']\n",
    "female = next(g for g in gender_data if g['group'] == 'Female')\n",
    "male = next(g for g in gender_data if g['group'] == 'Male')\n",
    "\n",
    "# Gather counts\n",
    "counts = [female['n_wrong'], male['n_wrong']]\n",
    "totals = [female['n_total'], male['n_total']]\n",
    "props = [counts[i] / totals[i] for i in range(2)]\n",
    "\n",
    "# Check if sample size conditions are met for z-test\n",
    "conditions_met = all([\n",
    "    totals[i] * props[i] >= 5 and totals[i] * (1 - props[i]) >= 5\n",
    "    for i in range(2)\n",
    "])\n",
    "\n",
    "# Perform test if valid\n",
    "if conditions_met:\n",
    "    stat, pval = proportions_ztest(counts, totals)\n",
    "    gender_result = {\n",
    "        \"test\": \"z-test for two proportions\",\n",
    "        \"z_statistic\": round(stat, 4),\n",
    "        \"p_value\": round(pval, 4),\n",
    "        \"female_error_rate\": round(props[0], 3),\n",
    "        \"male_error_rate\": round(props[1], 3),\n",
    "        \"sample_size_conditions_met\": True\n",
    "    }\n",
    "else:\n",
    "    result = {\n",
    "        \"error\": \"Sample size requirements not met for z-test\",\n",
    "        \"sample_size_conditions_met\": False\n",
    "    }\n",
    "\n",
    "gender_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 'z-test for two proportions',\n",
       " 'z_statistic': -1.0524,\n",
       " 'p_value': 0.2926,\n",
       " 'white_error_rate': 0.177,\n",
       " 'non_white_error_rate': 0.237,\n",
       " 'sample_size_conditions_met': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract race data for White and Non-White\n",
    "race_data = summary['race']\n",
    "white = next(r for r in race_data if r['group'].lower() == 'white')\n",
    "non_white = next(r for r in race_data if r['group'].lower() == 'non-white')\n",
    "\n",
    "# Gather counts\n",
    "counts = [white['n_wrong'], non_white['n_wrong']]\n",
    "totals = [white['n_total'], non_white['n_total']]\n",
    "props = [counts[i] / totals[i] for i in range(2)]\n",
    "\n",
    "# Check sample size conditions\n",
    "conditions_met_race = all([\n",
    "    totals[i] * props[i] >= 5 and totals[i] * (1 - props[i]) >= 5\n",
    "    for i in range(2)\n",
    "])\n",
    "\n",
    "# Perform test if valid\n",
    "if conditions_met_race:\n",
    "    stat, pval = proportions_ztest(counts, totals)\n",
    "    race_result = {\n",
    "        \"test\": \"z-test for two proportions\",\n",
    "        \"z_statistic\": round(stat, 4),\n",
    "        \"p_value\": round(pval, 4),\n",
    "        \"white_error_rate\": round(props[0], 3),\n",
    "        \"non_white_error_rate\": round(props[1], 3),\n",
    "        \"sample_size_conditions_met\": True\n",
    "    }\n",
    "else:\n",
    "    race_result = {\n",
    "        \"error\": \"Sample size requirements not met for z-test\",\n",
    "        \"sample_size_conditions_met\": False\n",
    "    }\n",
    "\n",
    "race_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 'Chi-square test of independence',\n",
       " 'chi2_statistic': 3.1602,\n",
       " 'p_value': 0.206,\n",
       " 'degrees_of_freedom': 2,\n",
       " 'age_groups_compared': ['60 - 79', '40 - 59', '20 - 39'],\n",
       " 'all_expected_freqs_>=5': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract age group data and filter valid ones\n",
    "age_data = summary['age_group']\n",
    "valid_groups = [g for g in age_data if g['group'] in ['20 - 39', '40 - 59', '60 - 79']]\n",
    "\n",
    "# Create contingency table: [correct, incorrect] for each group\n",
    "age_contingency = []\n",
    "age_labels = []\n",
    "\n",
    "for group in valid_groups:\n",
    "    correct = group['n_total'] - group['n_wrong']\n",
    "    incorrect = group['n_wrong']\n",
    "    age_contingency.append([correct, incorrect])\n",
    "    age_labels.append(group['group'])\n",
    "\n",
    "# Run chi-square test of independence\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(age_contingency)\n",
    "\n",
    "# Create expected frequencies DataFrame and check for < 5\n",
    "expected_df = pd.DataFrame(expected, columns=['Correct_exp', 'Incorrect_exp'], index=age_labels)\n",
    "expected_check = (expected_df < 5).any(axis=1)\n",
    "any_cell_under_5 = expected_check.any()\n",
    "\n",
    "# Format result\n",
    "age_result = {\n",
    "    \"test\": \"Chi-square test of independence\",\n",
    "    \"chi2_statistic\": round(chi2_stat, 4),\n",
    "    \"p_value\": round(p_val, 4),\n",
    "    \"degrees_of_freedom\": dof,\n",
    "    \"age_groups_compared\": age_labels,\n",
    "    \"all_expected_freqs_>=5\": not any_cell_under_5\n",
    "}\n",
    "\n",
    "age_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: raw p = 0.2447, FDR-corrected p = 0.2926, significant = False\n",
      "Race: raw p = 0.2926, FDR-corrected p = 0.2926, significant = False\n",
      "Age Group: raw p = 0.2060, FDR-corrected p = 0.2926, significant = False\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Raw p-values from your tests\n",
    "pvals = [gender_result['p_value'], race_result['p_value'], age_result['p_value']]\n",
    "labels = ['Gender', 'Race', 'Age Group']\n",
    "\n",
    "# Apply FDR correction (Benjamini-Hochberg)\n",
    "reject, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# Display results\n",
    "for i in range(len(pvals)):\n",
    "    print(f\"{labels[i]}: raw p = {pvals[i]:.4f}, FDR-corrected p = {pvals_corrected[i]:.4f}, significant = {reject[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.487766\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Raw p-value</th>\n",
       "      <th>FDR-adjusted p</th>\n",
       "      <th>Significant (FDR &lt; 0.05)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>const</td>\n",
       "      <td>-0.2303</td>\n",
       "      <td>0.6483</td>\n",
       "      <td>0.6483</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_Male</th>\n",
       "      <td>gender_Male</td>\n",
       "      <td>-0.2460</td>\n",
       "      <td>0.4356</td>\n",
       "      <td>0.5445</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race__white</th>\n",
       "      <td>race__white</td>\n",
       "      <td>-0.3206</td>\n",
       "      <td>0.3821</td>\n",
       "      <td>0.5445</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_group_40 - 59</th>\n",
       "      <td>age_group_40 - 59</td>\n",
       "      <td>-1.0503</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_group_60 - 79</th>\n",
       "      <td>age_group_60 - 79</td>\n",
       "      <td>-0.8444</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Variable  Coefficient  Raw p-value  \\\n",
       "const                          const      -0.2303       0.6483   \n",
       "gender_Male              gender_Male      -0.2460       0.4356   \n",
       "race__white              race__white      -0.3206       0.3821   \n",
       "age_group_40 - 59  age_group_40 - 59      -1.0503       0.0456   \n",
       "age_group_60 - 79  age_group_60 - 79      -0.8444       0.0762   \n",
       "\n",
       "                   FDR-adjusted p  Significant (FDR < 0.05)  \n",
       "const                      0.6483                     False  \n",
       "gender_Male                0.5445                     False  \n",
       "race__white                0.5445                     False  \n",
       "age_group_40 - 59          0.1905                     False  \n",
       "age_group_60 - 79          0.1905                     False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "df['race_'] = df['race'].replace({\n",
    "    'White': 'white',\n",
    "    'Black or African American': 'Non-white',\n",
    "    'American Indian or Alaska Native': 'Non-white',\n",
    "    'Asian': 'Non-white',\n",
    "    'Others': 'Non-white',\n",
    "    'Not Mentioned': 'Unknown'\n",
    "})\n",
    "\n",
    "# Convert misclassified column to binary (1 = misclassified, 0 = correct)\n",
    "df['error'] = df['misclassified_fusion'].astype(int)\n",
    "\n",
    "# Filter rows to include only the desired levels\n",
    "df_filtered = df[\n",
    "    df['gender'].isin(['Female', 'Male']) &\n",
    "    df['race_'].isin(['white', 'Non-white']) &\n",
    "    df['age_group'].isin(['20 - 39', '40 - 59', '60 - 79'])\n",
    "].copy()\n",
    "\n",
    "# One-hot encode categorical predictors (drop_first avoids multicollinearity)\n",
    "X = pd.get_dummies(df_filtered[['gender', 'race_', 'age_group']], drop_first=True)\n",
    "X = sm.add_constant(X)\n",
    "# Ensure all X columns are numeric\n",
    "X = X.astype(float)\n",
    "y = df_filtered['error']\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = sm.Logit(y, X).fit()\n",
    "pvals = model.pvalues\n",
    "\n",
    "# Apply Benjamini-Hochberg FDR correction\n",
    "reject, pvals_corrected, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "\n",
    "# Display results in a table\n",
    "results = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Coefficient': model.params.round(4),\n",
    "    'Raw p-value': pvals.round(4),\n",
    "    'FDR-adjusted p': pvals_corrected.round(4),\n",
    "    'Significant (FDR < 0.05)': reject\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import cochrans_q\n",
    "\n",
    "df_clinician_ratings = df[~df['neurologist_label_ray'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3018962/2558307256.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clinician_ratings['pred_park'] = (df_clinician_ratings['pred_score_fusion'] >= 0.5).astype(int)\n"
     ]
    }
   ],
   "source": [
    "ray_preds = np.asarray(df_clinician_ratings['neurologist_label_ray'])                                     \n",
    "ruth_preds = np.asarray(df_clinician_ratings['neurologist_label_ruth'])   \n",
    "jamie_preds = np.asarray(df_clinician_ratings['neurologist_label_jamie'])   \n",
    "\n",
    "group_prediction = (ray_preds + ruth_preds + jamie_preds >= 2).astype(int)\n",
    "\n",
    "# Then compare boots_park['PPV'] to boots_group['PPV']\n",
    "\n",
    "\n",
    "df_clinician_ratings['pred_park'] = (df_clinician_ratings['pred_score_fusion'] >= 0.5).astype(int)\n",
    "\n",
    "park_preds = np.asarray(df_clinician_ratings['pred_park'])   \n",
    "\n",
    "true_labels = np.asarray(df_clinician_ratings['true_label']) \n",
    "\n",
    "summary_ray, boots_ray = calculate_metrics_bootstrapped(true_labels, ray_preds)\n",
    "summary_ruth, boots_ruth = calculate_metrics_bootstrapped(true_labels, ruth_preds)\n",
    "summary_jamie, boots_jamie = calculate_metrics_bootstrapped(true_labels, jamie_preds)\n",
    "summary_park, boots_park = calculate_metrics_bootstrapped(true_labels, park_preds)\n",
    "summary_group, boots_group = calculate_metrics_bootstrapped(true_labels, group_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def compare_park_to_clinicians(boots_park, boots_clinicians: dict, metrics: list):\n",
    "    \"\"\"\n",
    "    Compare PARK's metrics to clinicians using one-sided Mann-Whitney U test (PARK < Clinician).\n",
    "    \n",
    "    Args:\n",
    "        boots_park: dict of bootstrapped metrics for PARK\n",
    "        boots_clinicians: dict of dicts (e.g., {'Ray': boots_ray, ...})\n",
    "        metrics: list of metric names (e.g., ['PPV', 'Sensitivity'])\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary of test results and a printed summary message.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    significantly_lower_metrics = []\n",
    "\n",
    "    for metric in metrics:\n",
    "        pvals = []\n",
    "        pairs = []\n",
    "        \n",
    "        # inside the for metric loop\n",
    "        for name, boots in boots_clinicians.items():\n",
    "            dist_park = np.array(boots_park[metric])\n",
    "            dist_clinician = np.array(boots[metric])\n",
    "\n",
    "            # Check for valid data\n",
    "            if np.all(np.isnan(dist_park)) or np.all(np.isnan(dist_clinician)):\n",
    "                pvals.append(np.nan)\n",
    "            else:\n",
    "                try:\n",
    "                    stat = mannwhitneyu(dist_park, dist_clinician, alternative='less')\n",
    "                    pvals.append(stat.pvalue)\n",
    "                except ValueError:\n",
    "                    pvals.append(np.nan)\n",
    "            \n",
    "            pairs.append(f\"PARK vs {name}\")\n",
    "        \n",
    "        # FDR correction\n",
    "        if all(np.isfinite(pvals)):\n",
    "            reject, pvals_fdr, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "        else:\n",
    "            reject = [False] * len(pvals)\n",
    "            pvals_fdr = [np.nan] * len(pvals)\n",
    "        \n",
    "        # Store result\n",
    "        results[metric] = {\n",
    "            'pairs': pairs,\n",
    "            'raw_pvals': pvals,\n",
    "            'fdr_pvals': pvals_fdr,\n",
    "            'reject': reject\n",
    "        }\n",
    "\n",
    "        # Track significance\n",
    "        if any(reject):\n",
    "            significantly_lower_metrics.append(metric)\n",
    "        \n",
    "        # Print detailed result per metric\n",
    "        print(f\"\\nMetric: {metric}\")\n",
    "        for i, pair in enumerate(pairs):\n",
    "            print(f\"{pair}: raw p = {pvals[i]:.4f}, FDR-adjusted p = {pvals_fdr[i]:.4f}, significant = {reject[i]}\")\n",
    "    \n",
    "    # Summary message\n",
    "    if significantly_lower_metrics:\n",
    "        print(f\"\\n⚠️ PARK showed significantly lower performance (FDR < 0.05) than at least one clinician for: {', '.join(significantly_lower_metrics)}\")\n",
    "    else:\n",
    "        print(\"\\n✅ PARK did not show significantly lower performance than any clinician for any metric (FDR-adjusted p > 0.05).\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric: Accuracy\n",
      "PARK vs Ray: raw p = 0.9999, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Ruth: raw p = 0.0001, FDR-adjusted p = 0.0003, significant = True\n",
      "PARK vs Jamie: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Group: raw p = 0.4405, FDR-adjusted p = 0.8810, significant = False\n",
      "\n",
      "Metric: Specificity\n",
      "PARK vs Ray: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Ruth: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Jamie: raw p = 0.3216, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Group: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "\n",
      "Metric: Sensitivity\n",
      "PARK vs Ray: raw p = 0.0000, FDR-adjusted p = 0.0000, significant = True\n",
      "PARK vs Ruth: raw p = 0.0000, FDR-adjusted p = 0.0000, significant = True\n",
      "PARK vs Jamie: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Group: raw p = 0.0002, FDR-adjusted p = 0.0002, significant = True\n",
      "\n",
      "Metric: PPV\n",
      "PARK vs Ray: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Ruth: raw p = 0.9964, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Jamie: raw p = 0.8298, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Group: raw p = 0.9992, FDR-adjusted p = 1.0000, significant = False\n",
      "\n",
      "Metric: NPV\n",
      "PARK vs Ray: raw p = 0.0015, FDR-adjusted p = 0.0029, significant = True\n",
      "PARK vs Ruth: raw p = 0.0000, FDR-adjusted p = 0.0000, significant = True\n",
      "PARK vs Jamie: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Group: raw p = 0.0281, FDR-adjusted p = 0.0375, significant = True\n",
      "\n",
      "Metric: F1 Score\n",
      "PARK vs Ray: raw p = 0.9568, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Ruth: raw p = 0.0000, FDR-adjusted p = 0.0000, significant = True\n",
      "PARK vs Jamie: raw p = 1.0000, FDR-adjusted p = 1.0000, significant = False\n",
      "PARK vs Group: raw p = 0.2236, FDR-adjusted p = 0.4472, significant = False\n",
      "\n",
      "⚠️ PARK showed significantly lower performance (FDR < 0.05) than at least one clinician for: Accuracy, Sensitivity, NPV, F1 Score\n"
     ]
    }
   ],
   "source": [
    "# Define the clinicians and the metrics to compare\n",
    "clinicians_boots = {\n",
    "    'Ray': boots_ray,\n",
    "    'Ruth': boots_ruth,\n",
    "    'Jamie': boots_jamie,\n",
    "    'Group': boots_group\n",
    "}\n",
    "metrics_to_compare = ['Accuracy', 'Specificity', 'Sensitivity', 'PPV', 'NPV', 'F1 Score']\n",
    "\n",
    "# Run the function\n",
    "results = compare_park_to_clinicians(boots_park, clinicians_boots, metrics_to_compare)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
